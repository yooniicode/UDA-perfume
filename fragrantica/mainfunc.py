import time
import traceback  # ìƒì„¸ ì˜¤ë¥˜ ì¶œë ¥ì„ ìœ„í•´ ì„í¬íŠ¸
import undetected_chromedriver as uc
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    NoSuchElementException,
    TimeoutException,
    ElementClickInterceptedException,
    WebDriverException,
)
import csv
import os
import sys
from tenacity import retry, stop_after_attempt, wait_exponential
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from queue import Queue
import random  # ëœë¤ ë”œë ˆì´ ë° UA ì„ íƒìš©

# -----------------------
# 1. ê¸°ë³¸ ì„¤ì • / ë¡œê·¸
# -----------------------

if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except (AttributeError, OSError):
        pass

logging.getLogger('selenium').setLevel(logging.WARNING)
logging.getLogger('urllib3').setLevel(logging.WARNING)
logging.getLogger('undetected_chromedriver').setLevel(logging.WARNING)
os.environ['WDM_LOG'] = '0'

# -----------------------
# 2. í¬ë¡¤ëŸ¬ ì„¤ì •
# -----------------------

# --- 2.1. ê¸°ë³¸ ì„¤ì • ---
SEARCH_KEYWORD = "lush"
PERFUME_CSV_FILE = f'fragrantica_perfumes_{SEARCH_KEYWORD.lower().replace(" ", "-")}.csv'
REVIEW_CSV_FILE = f'fragrantica_reviews_{SEARCH_KEYWORD.lower().replace(" ", "-")}.csv'

# [ìˆ˜ì •] ë”œë ˆì´ ë° ì›Œì»¤ ì„¤ì • (ë´‡ íƒì§€ íšŒí”¼ìš©)
RATE_LIMIT_DELAY_RANGE = (10.0, 20.0)  # 10ì´ˆ ~ 20ì´ˆ ì‚¬ì´ ëœë¤ ëŒ€ê¸°
MAX_WORKERS = 1  # â˜…â˜…â˜… ë°˜ë“œì‹œ 1ë¡œ ìœ ì§€ â˜…â˜…â˜…

USER_AGENT_LIST = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.t (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
]

# --- 2.2. CSV íŒŒì¼ í—¤ë” ---
PERFUME_FIELDNAMES = [
    'url', 'product_name', 'brand_name', 'target_gender', 'image_url',
    'top_notes', 'middle_notes', 'base_notes',
]
REVIEW_FIELDNAMES = [
    'product_name', 'review_content', 'review_date', 'reviewer_name'
]

# --- 2.3. ì„ íƒì (Selectors) ---
PRIMARY_PRODUCT_LINK_SELECTOR = (By.CSS_SELECTOR, "a.prefumeHbox")
FALLBACK_PRODUCT_LINK_SELECTOR = (By.CSS_SELECTOR, "a.perfumeHbox")
ALTERNATIVE_PRODUCT_LINK_SELECTOR = (By.CSS_SELECTOR, "div.perfume-card > a")

# [ì œí’ˆ ì •ë³´]
PRODUCT_NAME_H1_SELECTOR = (By.CSS_SELECTOR, 'h1[itemprop="name"]')
BRAND_NAME_SELECTOR = (By.CSS_SELECTOR, 'span[itemprop="brand"] a span')
TARGET_GENDER_SELECTOR = (By.CSS_SELECTOR, 'h1[itemprop="name"] small')
IMAGE_URL_SELECTOR = (By.CSS_SELECTOR, 'img[itemprop="image"]')

# [ë¦¬ë·° ì •ë³´]
REVIEW_HOLDER_SELECTOR = (By.ID, "all-reviews")
REVIEW_BODY_SELECTOR = (By.CSS_SELECTOR, "div[itemprop='reviewBody']")
REVIEW_CONTAINER_SELECTOR = (By.CSS_SELECTOR, 'div.fragrance-review-box[itemprop="review"]')
REVIEW_CONTENT_SELECTOR = (By.CSS_SELECTOR, 'div[itemprop="reviewBody"] p')
REVIEW_DATE_SELECTOR = (By.CSS_SELECTOR, 'span[itemprop="datePublished"]')
REVIEWER_NAME_SELECTOR = (By.CSS_SELECTOR, 'p > b > a[href*="member"]')

# --- 2.4. ìŠ¤ë ˆë“œ ë½ (Locks) ---
csv_lock = threading.Lock()
print_lock = threading.Lock()


# -----------------------
# 3. ë“œë¼ì´ë²„ í’€ í´ë˜ìŠ¤
# -----------------------

class DriverPool:
    """ë“œë¼ì´ë²„ë¥¼ ë¯¸ë¦¬ ìƒì„±í•˜ê³  ì¬ì‚¬ìš©í•˜ëŠ” í’€"""

    def __init__(self, size=3):
        self.pool = Queue(maxsize=size)
        self.size = size
        safe_print(f"\nğŸ”§ ë“œë¼ì´ë²„ í’€ ì´ˆê¸°í™” ì¤‘ ({size}ê°œ)...")
        for i in range(size):
            try:
                # [ìˆ˜ì •] ê° ë“œë¼ì´ë²„ì— ëœë¤ User-Agent í• ë‹¹
                user_agent = random.choice(USER_AGENT_LIST)
                driver = self._create_driver(user_agent=user_agent)
                self.pool.put(driver)
                safe_print(f"   âœ… ë“œë¼ì´ë²„ {i + 1}/{size} ìƒì„± ì™„ë£Œ (UA: {user_agent[:40]}...)")
                time.sleep(1)
            except Exception as e:
                safe_print(f"   âŒ ë“œë¼ì´ë²„ {i + 1} ìƒì„± ì‹¤íŒ¨: {repr(e)}")
        safe_print(f"âœ… ë“œë¼ì´ë²„ í’€ ì¤€ë¹„ ì™„ë£Œ\n")

    def _create_driver(self, user_agent=None):  # [ìˆ˜ì •] user_agent ì¸ìˆ˜ ì¶”ê°€
        """ë‹¨ì¼ ë“œë¼ì´ë²„ ìƒì„±"""
        options = uc.ChromeOptions()
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-extensions')
        options.add_argument('--log-level=3')

        if user_agent:
            options.add_argument(f'--user-agent={user_agent}')
        else:
            # ê¸°ë³¸ UA (í´ë°±)
            options.add_argument(
                '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                'AppleWebKit/537.36 (KHTML, like Gecko) '
                'Chrome/120.0.0.0 Safari/537.36'
            )

        driver = uc.Chrome(options=options, use_subprocess=False)
        driver.implicitly_wait(3)
        return driver

    def get(self):
        return self.pool.get()

    def put(self, driver):
        self.pool.put(driver)

    def close_all(self):
        while not self.pool.empty():
            try:
                driver = self.pool.get_nowait()
                driver.quit()
            except:
                pass


# -----------------------
# 4. í—¬í¼ í•¨ìˆ˜
# -----------------------

def setup_csv_files():
    """CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë”ì™€ í•¨ê»˜ ìƒì„±."""
    try:
        if not os.path.exists(PERFUME_CSV_FILE):
            with open(PERFUME_CSV_FILE, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=PERFUME_FIELDNAMES)
                writer.writeheader()
        if not os.path.exists(REVIEW_CSV_FILE):
            with open(REVIEW_CSV_FILE, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=REVIEW_FIELDNAMES)
                writer.writeheader()
    except PermissionError as e:
        print("\n" + "!" * 60)
        print(f"âŒ [ì¹˜ëª…ì  ì˜¤ë¥˜] íŒŒì¼ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {e}")
        print(f"   '{PERFUME_CSV_FILE}' ë˜ëŠ” '{REVIEW_CSV_FILE}' íŒŒì¼ì´")
        print("   Excel ë“± ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì—ì„œ ì—´ë ¤ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ëª¨ë‘ ë‹«ì€ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        print("!" * 60 + "\n")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ì„¤ì • ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


def click_with_js(driver, element):
    try:
        driver.execute_script("arguments[0].click();", element)
    except Exception:
        pass


def safe_find_text(driver_or_element, *selector, wait_time=2, default=""):
    try:
        element = WebDriverWait(driver_or_element, wait_time).until(
            EC.presence_of_element_located(selector)
        )
        return element.text.strip()
    except (NoSuchElementException, TimeoutException):
        return default


def safe_find_attr(driver_or_element, *selector, attr="src", wait_time=2, default=""):
    try:
        element = WebDriverWait(driver_or_element, wait_time).until(
            EC.presence_of_element_located(selector)
        )
        return element.get_attribute(attr)
    except (NoSuchElementException, TimeoutException):
        return default


def get_notes_by_type(driver, note_type):
    """ 'Top Notes', 'Middle Notes', 'Base Notes' í—¤ë”ë¡œ ë…¸íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤. """
    notes = []
    try:
        xpath = f"//h4[b='{note_type} Notes']/following-sibling::div[1]//div[contains(@style, 'margin')]/div[last()]"
        note_elements = driver.find_elements(By.XPATH, xpath)
        notes = [elem.text.strip() for elem in note_elements if elem.text.strip()]
    except Exception:
        pass
    return ", ".join(notes) if notes else ""


def get_undivided_notes(driver):
    """ 'Fragrance Notes' (í†µí•©) í—¤ë”ë¡œ ë…¸íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤. """
    notes = []
    try:
        xpath = (
            "//span[contains(., 'Fragrance Notes')]/following::div"
            "[contains(@style, 'flex-flow: wrap') or contains(@style, 'flex-wrap: wrap')][1]"
            "/.//div[contains(@style, 'margin')]/div[last()]"
        )

        note_elements = driver.find_elements(By.XPATH, xpath)

        if not note_elements:
            xpath_h4 = (
                "//h4[b='Fragrance Notes']/following-sibling::div[1]"
                "/.//div[contains(@style, 'margin')]/div[last()]"
            )
            note_elements = driver.find_elements(By.XPATH, xpath_h4)

        notes = [elem.text.strip() for elem in note_elements if elem.text.strip()]
    except Exception:
        pass

    return ", ".join(notes) if notes else ""


def write_batch_to_csv(filename, fieldnames, data_batch):
    if not data_batch:
        return
    with csv_lock:
        with open(filename, 'a', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writerows(data_batch)


def safe_print(message):
    with print_lock:
        print(message)


# -----------------------
# 5. URL ìˆ˜ì§‘ í•¨ìˆ˜
# -----------------------

def collect_all_product_urls(start_url, max_same_rounds=3, wait_between_scrolls=2.0):
    """Designers í˜ì´ì§€ì—ì„œ ëª¨ë“  ì œí’ˆ URL ìˆ˜ì§‘"""
    safe_print(f"ğŸš€ [1ë‹¨ê³„] '{start_url}'ì—ì„œ URL ìˆ˜ì§‘ ì‹œì‘...")
    options = uc.ChromeOptions()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--start-maximized')
    options.add_argument(f'--user-agent={random.choice(USER_AGENT_LIST)}')

    driver = uc.Chrome(options=options, use_subprocess=False)
    wait = WebDriverWait(driver, 15)

    all_product_urls_set = set()

    selectors_to_try = [
        PRIMARY_PRODUCT_LINK_SELECTOR,
        FALLBACK_PRODUCT_LINK_SELECTOR,
        ALTERNATIVE_PRODUCT_LINK_SELECTOR
    ]
    selector_in_use = None

    try:
        driver.get(start_url)
        safe_print(f"âœ… '{start_url}' ì ‘ì† ì™„ë£Œ")

        for i, selector in enumerate(selectors_to_try):
            try:
                wait.until(EC.presence_of_element_located(selector))
                selector_in_use = selector
                safe_print(f"ğŸ” ì„ íƒì #{i + 1} ë¡œ ì œí’ˆ ìš”ì†Œ í™•ì¸ë¨.")
                break
            except TimeoutException:
                safe_print(f"âš ï¸ ì„ íƒì #{i + 1} ì—†ìŒ. ë‹¤ìŒ ì‹œë„...")

        if not selector_in_use:
            safe_print("âŒ ëª¨ë“  ì„ íƒìë¡œ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í•¨. selectorë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
            return []

        pagination_links = driver.find_elements(By.CSS_SELECTOR, 'div.pagination a')

        if not pagination_links:
            # --- Dior ë°©ì‹ (ë¬´í•œ ìŠ¤í¬ë¡¤) ---
            safe_print("   (i) 'ë¬´í•œ ìŠ¤í¬ë¡¤' ë°©ì‹ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤")
            prev_count = 0
            same_rounds = 0
            last_height = driver.execute_script("return document.body.scrollHeight")
            while True:
                try:
                    elements = driver.find_elements(*selector_in_use)
                    if not elements and prev_count == 0:
                        try:
                            wait.until(EC.presence_of_element_located(selector_in_use))
                            elements = driver.find_elements(*selector_in_use)
                        except TimeoutException:
                            safe_print("... ì•„ì§ ì œí’ˆ ìš”ì†Œê°€ ì—†ìŒ (ì ì‹œ í›„ ì¬ì‹œë„)")

                    page_urls = [e.get_attribute('href') for e in elements if e.get_attribute('href')]
                    newly_found = set(page_urls) - all_product_urls_set
                    if newly_found:
                        all_product_urls_set.update(newly_found)
                        safe_print(f"â• ìƒˆ URL {len(newly_found)}ê°œ ë°œê²¬ (ëˆ„ì : {len(all_product_urls_set)})")

                    if elements:
                        driver.execute_script("arguments[0].scrollIntoView({behavior:'smooth', block:'end'});",
                                              elements[-1])
                    else:
                        driver.execute_script("window.scrollBy(0, window.innerHeight);")

                    time.sleep(wait_between_scrolls)

                    try:
                        WebDriverWait(driver, 6).until(
                            lambda d: len(d.find_elements(*selector_in_use)) > prev_count
                        )
                        prev_count = len(driver.find_elements(*selector_in_use))
                        same_rounds = 0
                        safe_print("ğŸ”„ ìš”ì†Œ ìˆ˜ ì¦ê°€ í™•ì¸ â€” ê³„ì† ìˆ˜ì§‘")
                    except TimeoutException:
                        same_rounds += 1
                        safe_print(f"â± ë³€í™” ì—†ìŒ (ì—°ì† {same_rounds}/{max_same_rounds})")

                    new_height = driver.execute_script("return document.body.scrollHeight")
                    if new_height == last_height:
                        same_rounds += 1
                        safe_print(f"ğŸ“ í˜ì´ì§€ ë†’ì´ ë³€í™” ì—†ìŒ (ì—°ì† ì¦ê°€ ì²´í¬: {same_rounds})")
                    else:
                        last_height = new_height
                        same_rounds = 0

                    if same_rounds >= max_same_rounds:
                        safe_print("ğŸ ë” ì´ìƒì˜ ì½˜í…ì¸  ë¡œë“œ ì—†ìŒìœ¼ë¡œ íŒë‹¨. ìˆ˜ì§‘ ì¢…ë£Œ.")
                        break
                except Exception as e:
                    safe_print(f"âš ï¸ ë¬´í•œ ìŠ¤í¬ë¡¤ ì¤‘ ì˜ˆì™¸: {repr(e)}")
                    break
        else:
            safe_print("   (i) 'í˜ì´ì§€ë„¤ì´ì…˜' ë°©ì‹ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤")
            page_num = 1
            while True:
                try:
                    wait.until(EC.presence_of_element_located(selector_in_use))
                    elements = driver.find_elements(*selector_in_use)

                    page_urls = []
                    for elem in elements:
                        href = elem.get_attribute('href')
                        if href and href.startswith("https://www.fragrantica.com/perfume/"):
                            page_urls.append(href)

                    new_urls_count = len(set(page_urls) - all_product_urls_set)
                    all_product_urls_set.update(page_urls)
                    safe_print(f"ğŸ“„ í˜ì´ì§€ {page_num}: {new_urls_count}ê°œ ì‹ ê·œ ìˆ˜ì§‘ (ëˆ„ì : {len(all_product_urls_set)}ê°œ)")

                except TimeoutException:
                    safe_print(f"âš ï¸  í˜ì´ì§€ {page_num}ì—ì„œ ì œí’ˆ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")

                try:
                    next_button = wait.until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, 'a[aria-label="Next Â»"]'))
                    )
                    click_with_js(driver, next_button)
                    time.sleep(1.5)
                    page_num += 1
                except (TimeoutException, NoSuchElementException):
                    safe_print("ğŸ ë” ì´ìƒ 'ë‹¤ìŒ' í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. URL ìˆ˜ì§‘ ì¢…ë£Œ.")
                    break

    except Exception as e:
        safe_print(f"âŒ URL ìˆ˜ì§‘ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {repr(e)}")
        traceback.print_exc()
    finally:
        safe_print("====== ğŸ”§ URL ìˆ˜ì§‘ ë“œë¼ì´ë²„ ì¢…ë£Œ ======")
        driver.quit()

    return list(all_product_urls_set)


# -----------------------
# 6. í•µì‹¬ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜
# -----------------------

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def scrape_product_details(driver, url):
    """
    ì œí’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ í–¥ìˆ˜ ì •ë³´ë¥¼ ìŠ¤í¬ë©.
    [ìˆ˜ì •] T/M/B ë…¸íŠ¸ê°€ ì—†ëŠ” ê²½ìš°, 'Fragrance Notes' (í†µí•©)ë¥¼ middle_notesë¡œ ì €ì¥
    """
    wait = WebDriverWait(driver, 10)

    h1_element = wait.until(
        EC.presence_of_element_located(PRODUCT_NAME_H1_SELECTOR)
    )

    product_name = driver.execute_script(
        "return arguments[0].firstChild.textContent.trim()", h1_element
    )
    brand_name = safe_find_text(h1_element, *BRAND_NAME_SELECTOR, default=SEARCH_KEYWORD.title())
    target_gender = safe_find_text(h1_element, *TARGET_GENDER_SELECTOR, default="NA")

    image_url = safe_find_attr(driver, *IMAGE_URL_SELECTOR, attr="src", default="")


    # 1. í‘œì¤€ T/M/B ë…¸íŠ¸ë¥¼ ë¨¼ì € ì‹œë„
    top_notes = get_notes_by_type(driver, "Top")
    middle_notes = get_notes_by_type(driver, "Middle")
    base_notes = get_notes_by_type(driver, "Base")

    # 2. ë§Œì•½ T/M/Bê°€ ëª¨ë‘ ë¹„ì–´ìˆë‹¤ë©´, 'Fragrance Notes' (í†µí•©) ì¼€ì´ìŠ¤ë¥¼ ì‹œë„
    if not top_notes and not middle_notes and not base_notes:
        safe_print(f"      ... {product_name}: T/M/B ë…¸íŠ¸ ì—†ìŒ. 'Fragrance Notes' í†µí•© ê²€ìƒ‰ ì‹œë„...")
        # [ì‹ ê·œ] í—¬í¼ í•¨ìˆ˜ í˜¸ì¶œ
        undivided_notes = get_undivided_notes(driver)

        if undivided_notes:
            # ìš”ì²­ëŒ€ë¡œ undivided_notesë¥¼ middle_notesì— í• ë‹¹
            middle_notes = undivided_notes
            safe_print(f"      ... {product_name}: í†µí•© ë…¸íŠ¸ ë°œê²¬. Middleì— ì €ì¥.")

    product_data = {
        'url': url,
        'product_name': product_name,
        'brand_name': brand_name,
        'target_gender': target_gender,
        'image_url': image_url,
        'top_notes': top_notes,
        'middle_notes': middle_notes,
        'base_notes': base_notes,
    }

    return product_name, product_data


def scrape_reviews(driver, product_name):
    """
    [7ì°¨ ìˆ˜ì •] 'all-reviews' ì„¹ì…˜ ê°ì§€ í›„, ë¦¬ë·° 'ì»¨í…Œì´ë„ˆ'ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
    """
    reviews_batch = []
    processed_review_identifiers = set()

    try:
        # 1. 'all-reviews' ì„¹ì…˜ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ (ìµœëŒ€ 12ë²ˆ) ìŠ¤í¬ë¡¤
        safe_print(f"      ... {product_name}: 'all-reviews' ì„¹ì…˜ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ìŠ¤í¬ë¡¤...")
        reviews_section = None
        max_scroll_attempts = 12

        for attempt in range(max_scroll_attempts):
            try:
                # 0.5ì´ˆì˜ ì§§ì€ ëŒ€ê¸° ì‹œê°„ìœ¼ë¡œ 'all-reviews' ìš”ì†Œë¥¼ ì°¾ì•„ë´„
                reviews_section = WebDriverWait(driver, 0.5).until(
                    EC.presence_of_element_located(REVIEW_HOLDER_SELECTOR)
                )
                safe_print(f"      ... {product_name}: ìŠ¤í¬ë¡¤ {attempt + 1}íšŒ ë§Œì— ì„¹ì…˜ ë°œê²¬!")
                break  # ì°¾ì•˜ìœ¼ë©´ ë£¨í”„ íƒˆì¶œ
            except TimeoutException:
                # ëª» ì°¾ì•˜ìœ¼ë©´ í•œ í™”ë©´ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤
                driver.execute_script("window.scrollBy(0, window.innerHeight * 0.9);")
                time.sleep(0.7)  # JSê°€ ë°˜ì‘í•  ì‹œê°„

        # 2. 12ë²ˆ ìŠ¤í¬ë¡¤ í›„ì—ë„ ëª» ì°¾ì•˜ìœ¼ë©´ ë¦¬ë·° 0ê°œë¡œ ì²˜ë¦¬
        if not reviews_section:
            safe_print(f"      â„¹ï¸  {product_name}: {max_scroll_attempts}íšŒ ìŠ¤í¬ë¡¤ í›„ì—ë„ ë¦¬ë·° ì„¹ì…˜ ì—†ìŒ -> ë¦¬ë·° 0ê°œ")
            return []

        # 3. ì„¹ì…˜ì„ ì°¾ì•˜ìœ¼ë‹ˆ í•´ë‹¹ ìœ„ì¹˜ë¡œ ì •í™•íˆ ì´ë™
        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", reviews_section)
        time.sleep(1)

        # 4. ë¬´í•œ ìŠ¤í¬ë¡¤ ë£¨í”„ ì‹œì‘ (ì²« ëŒ€ê¸° ë¡œì§ì„ ë£¨í”„ ì•ˆìœ¼ë¡œ ì´ë™)
        while True:

            # 5. ì²« ì‹œë„(ë¦¬ë·°ê°€ 0ê°œ)ì¼ ê²½ìš°, ë¦¬ë·° 'ì»¨í…Œì´ë„ˆ'ê°€ ë¡œë“œë  ë•Œê¹Œì§€ 15ì´ˆê°„ ëŒ€ê¸°
            if not processed_review_identifiers:
                try:
                    # 'all-reviews' ì„¹ì…˜ì´ ìˆìœ¼ë‹ˆ, 'review-box'ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ 15ì´ˆ ëŒ€ê¸°
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located(REVIEW_CONTAINER_SELECTOR)
                    )
                    safe_print(f"      âœ” {product_name}: ë¦¬ë·° ë¸”ë¡ ê°ì§€ë¨! ì¶”ì¶œ ì‹œì‘")
                except TimeoutException:
                    # 15ì´ˆë¥¼ ê¸°ë‹¤ë ¤ë„ 'review-box'ê°€ ì•ˆ ëœ¨ë©´, ì •ë§ ë¦¬ë·°ê°€ 0ê°œì¸ ê²ƒì„
                    safe_print(f"      â„¹ï¸  {product_name}: ì„¹ì…˜ì€ ìˆìœ¼ë‚˜ 15ì´ˆ ë‚´ ë¦¬ë·° ë¡œë“œ ì•ˆë¨ (ë¦¬ë·° 0ê°œ).")
                    break

            count_before_batch = len(processed_review_identifiers)
            review_elements = driver.find_elements(*REVIEW_CONTAINER_SELECTOR)
            new_reviews_found_this_scroll = False

            for review in review_elements:
                try:
                    # ê³ ìœ  ID ìƒì„± ë° ì¤‘ë³µ í™•ì¸
                    reviewer_name_text = safe_find_text(review, *REVIEWER_NAME_SELECTOR, wait_time=0.1, default="Guest")
                    review_date_text = safe_find_text(review, *REVIEW_DATE_SELECTOR, wait_time=0.1, default="NA")
                    content_preview = safe_find_text(review, *REVIEW_CONTENT_SELECTOR, wait_time=0.1, default="")[:20]

                    unique_id = (reviewer_name_text, review_date_text, content_preview)

                    if unique_id in processed_review_identifiers:
                        continue

                    processed_review_identifiers.add(unique_id)
                    new_reviews_found_this_scroll = True

                    # ë‚´ìš© ì¶”ì¶œ
                    content_elements = review.find_elements(*REVIEW_CONTENT_SELECTOR)
                    content = " ".join([p.text.strip() for p in content_elements if p.text.strip()])

                    if content:
                        reviews_batch.append({
                            'product_name': product_name,
                            'review_content': content,
                            'review_date': review_date_text,
                            'reviewer_name': reviewer_name_text,
                        })
                except Exception:
                    continue

            if new_reviews_found_this_scroll or count_before_batch == 0:
                safe_print(f"      ğŸ“ {product_name}: {len(reviews_batch)}ê°œ ìˆ˜ì§‘ë¨...")

            # ì¢…ë£Œ ì¡°ê±´ 1: ìƒˆ ë¦¬ë·° ì—†ìŒ
            if not new_reviews_found_this_scroll and count_before_batch > 0:
                safe_print(f"      ğŸ {product_name}: ë” ì´ìƒ ìƒˆ ë¦¬ë·° ì—†ìŒ. ì¢…ë£Œ.")
                break

            # ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•´ ë§ˆì§€ë§‰ ìš”ì†Œë¡œ ìŠ¤í¬ë¡¤
            try:
                last_element = review_elements[-1]
                driver.execute_script("arguments[0].scrollIntoView(true);", last_element)
            except IndexError:
                # ìŠ¤í¬ë¡¤í•  ìš”ì†Œê°€ ì—†ìŒ (ì²« ëŒ€ê¸°ì—ì„œ 0ê°œë©´ ì´ë¯¸ breakë¨)
                break

                # ì¢…ë£Œ ì¡°ê±´ 2: DOM ìš”ì†Œ ê°œìˆ˜ ë³€í™” ëŒ€ê¸° (8ì´ˆ)
            try:
                current_total = len(review_elements)
                WebDriverWait(driver, 8).until(
                    lambda d: len(d.find_elements(*REVIEW_CONTAINER_SELECTOR)) > current_total
                )
            except TimeoutException:
                safe_print(f"      ğŸ {product_name}: ì¶”ê°€ ë¡œë”© ì—†ìŒ. ìˆ˜ì§‘ ì™„ë£Œ.")
                break

    except Exception as e:
        safe_print(f"      âŒ {product_name}: ë¦¬ë·° ìˆ˜ì§‘ ì¤‘ ë¡œì§ ì—ëŸ¬: {repr(e)}")
        traceback.print_exc()  # ìƒì„¸ ì˜¤ë¥˜ í™•ì¸

    safe_print(f"      âœ… {product_name}: ì´ {len(reviews_batch)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ")
    return reviews_batch


# -----------------------
# 7. ì›Œì»¤ í•¨ìˆ˜
# -----------------------

def process_single_product(args, driver_pool):
    """
    ë‹¨ì¼ ì œí’ˆ ì²˜ë¦¬.
    â˜…â˜… ì‘ì—… ì‹œì‘ 'ì „'ì— íœ´ì‹ ë¡œì§ì„ ë¨¼ì € ìˆ˜í–‰ â˜…â˜…
    """
    url, index, total = args
    driver = None
    product_name = url.split('/')[-1]

    # --- ì „ëµì  íœ´ì‹ ë¡œì§ (ì›Œì»¤ ìŠ¤ë ˆë“œê°€ ì§ì ‘ ìˆ˜í–‰) ---
    # 1-based indexì´ë¯€ë¡œ, (index - 1)ì´ 40ì˜ ë°°ìˆ˜ì¼ ë•Œ íœ´ì‹
    # (ì¦‰, 41ë²ˆì§¸, 81ë²ˆì§¸... ì‘ì—…ì„ ì‹œì‘í•˜ê¸° 'ì „'ì— íœ´ì‹)
    break_point = 40
    sleep_time_sec = 600  # 10ë¶„

    # (index - 1)ì´ 0ë³´ë‹¤ í¬ê³ , break_pointì˜ ë°°ìˆ˜ì¼ ë•Œ
    if (index - 1) > 0 and (index - 1) % break_point == 0:
        safe_print("\n" + "=" * 60)
        safe_print(f"â˜•ï¸ [ì „ëµì  íœ´ì‹] {index - 1}ê°œ ì²˜ë¦¬ ì™„ë£Œ. ë´‡ íƒì§€ íšŒí”¼ë¥¼ ìœ„í•´ {sleep_time_sec / 60:.0f}ë¶„ê°„ íœ´ì‹í•©ë‹ˆë‹¤.")
        safe_print(f"   (í˜„ì¬ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')})")
        print("=" * 60 + "\n")

        time.sleep(sleep_time_sec)  # â˜…â˜…â˜… ì‘ì—… ìŠ¤ë ˆë“œ(ì›Œì»¤)ê°€ ì§ì ‘ íœ´ì‹ â˜…â˜…â˜…

        safe_print(f"âœ… íœ´ì‹ ì™„ë£Œ. ë‹¤ìŒ ì‘ì—…({index}/{total})ì„ ì¬ê°œí•©ë‹ˆë‹¤...\n")

    try:
        driver = driver_pool.get()
        driver.get(url)

        product_name, product_data = scrape_product_details(driver, url)
        write_batch_to_csv(PERFUME_CSV_FILE, PERFUME_FIELDNAMES, [product_data])

        reviews_batch = scrape_reviews(driver, product_name)
        if reviews_batch:
            write_batch_to_csv(REVIEW_CSV_FILE, REVIEW_FIELDNAMES, reviews_batch)

        # ê³ ì • ë”œë ˆì´ ëŒ€ì‹  ëœë¤ ë”œë ˆì´ ì ìš©
        delay = random.uniform(*RATE_LIMIT_DELAY_RANGE)
        safe_print(f"      ... ë‹¤ìŒ ì‘ì—…ê¹Œì§€ {delay:.1f}ì´ˆ ëŒ€ê¸° ...")
        time.sleep(delay)

        driver_pool.put(driver)

        return {
            'status': 'success',
            'product_name': product_name,
            'review_count': len(reviews_batch),
            'index': index,
            'total': total
        }

    except Exception as e:
        if driver:
            safe_print(f"  (i) {product_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ë“œë¼ì´ë²„ ì¬ì‹œì‘...")
            try:
                driver.quit()
            except:
                pass

            try:
                # ë“œë¼ì´ë²„ ì¬ìƒì„± ì‹œì—ë„ ëœë¤ UA ì ìš©
                new_user_agent = random.choice(USER_AGENT_LIST)
                new_driver = driver_pool._create_driver(user_agent=new_user_agent)
                driver_pool.put(new_driver)
                safe_print(f"  (i) ìƒˆ ë“œë¼ì´ë²„ ìƒì„± í›„ í’€ì— ë°˜í™˜ ì™„ë£Œ.")
            except Exception as e_create:
                safe_print(f"  (E) ìƒˆ ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e_create}.")
                pass

        return {
            'status': 'failed',
            'error': repr(e)[:120],
            'url': url,
            'index': index,
            'total': total
        }


# -----------------------
# 8. ë©”ì¸ ì‹¤í–‰
# -----------------------

def get_already_scraped_urls(csv_file):
    """
    CSV íŒŒì¼ì„ ì½ì–´ ì´ë¯¸ ìˆ˜ì§‘ëœ URL ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    scraped_urls = set()
    if not os.path.exists(csv_file):
        return scraped_urls

    try:
        with open(csv_file, 'r', encoding='utf-8-sig') as f:
            # í—¤ë”ë¥¼ ê±´ë„ˆë›°ê³  ì½ê¸° ìœ„í•´ DictReader ì‚¬ìš©
            reader = csv.DictReader(f)
            for row in reader:
                # 'url' ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ê³  ê°’ì´ ìˆì„ ê²½ìš°ì—ë§Œ ì¶”ê°€
                if 'url' in row and row['url']:
                    scraped_urls.add(row['url'])
    except FileNotFoundError:
        pass  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ set ë°˜í™˜
    except Exception as e:
        safe_print(f"âš ï¸ ê¸°ì¡´ CSV íŒŒì¼({csv_file}) ì½ê¸° ì˜¤ë¥˜: {e}")
        # íŒŒì¼ì´ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì•ˆì „ì„ ìœ„í•´ ë¹ˆ set ë°˜í™˜
        pass

    return scraped_urls


def main():
    """
    1. 'ì´ì–´ê°€ê¸°' ë¡œì§ ì¶”ê°€ (ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€)
    2. 'ì „ëµì  íœ´ì‹' ë¡œì§ì„ process_single_product í•¨ìˆ˜ë¡œ ì´ë™ì‹œí‚´
    """
    start_time = time.time()

    print("=" * 60)
    print(f"ğŸš€ Fragrantica í¬ë¡¤ëŸ¬ ì‹œì‘ (í‚¤ì›Œë“œ: {SEARCH_KEYWORD})")
    print(f"   (ë“œë¼ì´ë²„ í’€: {MAX_WORKERS}ê°œ, ë”œë ˆì´: {RATE_LIMIT_DELAY_RANGE[0]}~{RATE_LIMIT_DELAY_RANGE[1]}ì´ˆ)")
    print("=" * 60)

    # --- 1. CSV íŒŒì¼ ì¤€ë¹„ ---
    setup_csv_files()

    # --- 2. 'ì´ì–´ê°€ê¸°' ë¡œì§: ì´ë¯¸ ìˆ˜ì§‘í•œ URL ë¶ˆëŸ¬ì˜¤ê¸° ---
    already_scraped_urls = get_already_scraped_urls(PERFUME_CSV_FILE)
    if already_scraped_urls:
        print(f"âœ… [ì´ì–´ê°€ê¸°] ê¸°ì¡´ì— ìˆ˜ì§‘í•œ {len(already_scraped_urls)}ê°œì˜ URLì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")

    # --- 3. URL ìˆ˜ì§‘ ---
    formatted_keyword = SEARCH_KEYWORD.title().replace(" ", "-")
    start_url = f"https://www.fragrantica.com/designers/{formatted_keyword}.html"

    url_collection_start = time.time()
    all_product_urls = collect_all_product_urls(start_url)
    url_collection_time = time.time() - url_collection_start

    if not all_product_urls:
        print(f"âŒ '{SEARCH_KEYWORD}'ì— ëŒ€í•œ URLì´ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # --- 4. 'ì´ì–´ê°€ê¸°' ë¡œì§: ìˆ˜ì§‘í•  URL í•„í„°ë§ ---
    urls_to_scrape = [url for url in all_product_urls if url not in already_scraped_urls]

    print(f"\nâœ… ì´ {len(all_product_urls)}ê°œ ì œí’ˆ ë°œê²¬ (ì†Œìš” ì‹œê°„: {url_collection_time:.1f}ì´ˆ)")
    print(f"   - ì´ë¯¸ ìˆ˜ì§‘ëœ URL: {len(already_scraped_urls)}ê°œ")
    print(f"   - â—ï¸ ìƒˆë¡œ ìˆ˜ì§‘í•  URL: {len(urls_to_scrape)}ê°œ")

    if not urls_to_scrape:
        print("\nğŸ‰ ëª¨ë“  ì œí’ˆ ìˆ˜ì§‘ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # --- 5. ì˜ˆìƒ ì‹œê°„ ê³„ì‚° (ìƒˆë¡œ ìˆ˜ì§‘í•  URL ê¸°ì¤€) ---
    avg_delay = sum(RATE_LIMIT_DELAY_RANGE) / 2
    avg_time_per_product = 8 + avg_delay

    # íœ´ì‹ ì‹œê°„ ê³„ì‚° (40ê°œë‹¹ 10ë¶„(600ì´ˆ) íœ´ì‹)
    total_rests = (len(urls_to_scrape) // 40)
    total_rest_time = total_rests * 600

    estimated_time_total = (len(urls_to_scrape) * avg_time_per_product) + total_rest_time

    print(f"\nğŸ“Š ì˜ˆìƒ ì†Œìš” ì‹œê°„ (ë”œë ˆì´ {avg_delay:.1f}ì´ˆ + íœ´ì‹ {total_rests}íšŒ í¬í•¨):")
    print(f"   ì•½ {estimated_time_total / 60:.1f}ë¶„ (ë˜ëŠ” {estimated_time_total / 3600:.2f} ì‹œê°„)")

    # --- 6. ë“œë¼ì´ë²„ í’€ ë° ìŠ¤í¬ë˜í•‘ ì‹œì‘ ---
    driver_pool = DriverPool(size=MAX_WORKERS)

    print("\n[2ë‹¨ê³„] ì œí’ˆ ìŠ¤í¬ë˜í•‘ ì‹œì‘ (ì´ì–´ê°€ê¸° ëª¨ë“œ)...")
    print("-" * 60)

    scraping_start = time.time()
    total = len(urls_to_scrape)
    tasks = [(url, i + 1, total) for i, url in enumerate(urls_to_scrape)]

    success_count = 0
    failed_count = 0

    # íœ´ì‹ ì¹´ìš´í„°ê°€ ë” ì´ìƒ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì‚­ì œ
    # tasks_since_last_break = 0

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {
            executor.submit(process_single_product, task, driver_pool): task
            for task in tasks
        }

        for future in as_completed(futures):
            result = future.result()
            percentage = (result['index'] / result['total']) * 100

            if result['status'] == 'success':
                success_count += 1
                # tasks_since_last_break += 1 # <-- ì‚­ì œ

                if result['review_count'] > 0:
                    safe_print(
                        f"[{result['index']}/{result['total']} ({percentage:.1f}%)] âœ… {result['product_name']} - ë¦¬ë·° {result['review_count']}ê°œ")
                else:
                    safe_print(
                        f"[{result['index']}/{result['total']} ({percentage:.1f}%)] âœ… {result['product_name']} - ì œí’ˆ ì •ë³´ë§Œ")


            else:
                failed_count += 1
                safe_print(
                    f"[{result['index']}/{result['total']} ({percentage:.1f}%)] âŒ ì²˜ë¦¬ ì‹¤íŒ¨ - {result['url']} - {result['error']}")

    print("\nğŸ”§ ë“œë¼ì´ë²„ í’€ ì¢…ë£Œ ì¤‘...")
    driver_pool.close_all()

    scraping_time = time.time() - scraping_start
    total_time = time.time() - start_time

    print("-" * 60)
    print("\n" + "=" * 60)
    print("âœ… ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nğŸ“Š í†µê³„:")
    print(f"   - ì´ {len(urls_to_scrape)}ê°œ ì¤‘ {success_count}ê°œ ì„±ê³µ")
    print(f"   - ì‹¤íŒ¨: {failed_count}ê°œ")
    print(f"\nâ±ï¸  ì†Œìš” ì‹œê°„:")
    print(f"   - URL ìˆ˜ì§‘: {url_collection_time:.1f}ì´ˆ")
    print(f"   - ì œí’ˆ ìŠ¤í¬ë˜í•‘ (íœ´ì‹ ì‹œê°„ í¬í•¨): {scraping_time / 60:.1f}ë¶„")
    print(f"   - ì „ì²´: {total_time / 60:.1f}ë¶„")
    print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
    print(f"   - {PERFUME_CSV_FILE}")
    print(f"   - {REVIEW_CSV_FILE}")
    print("=" * 60)


if __name__ == "__main__":
    main()